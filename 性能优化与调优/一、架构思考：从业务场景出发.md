## 一、架构思考：从业务场景出发

在设计之初，我们就要明确一点：**没有完美的架构，只有最适合业务的架构**。

一个典型的临床研究智能监测系统，它的读写模型有以下特点：

*   **写操作**：数据录入（CRF表单填写）、患者日记提交、稽查痕迹记录等。这些操作对**数据一致性**要求极高，写入必须成功，且后续的读取必须能立刻看到最新结果。
*   **读操作**：数据报表生成、多维度数据查询、研究进度看板展示等。这些操作的特点是**高并发、可容忍秒级延迟**。研究者看一个延迟 1-2 秒的报表，通常是可以接受的。

这种“写少读多，写要求强一致，读要求高并发”的场景，就是读写分离的最佳实践土壤。

我们采用的是经典的 **MySQL 主从（Master-Slave）架构**：

*   **主库 (Master)**：承担所有写操作（INSERT, UPDATE, DELETE）。它是唯一的数据入口，保证了数据源的统一和一致性。
*   **从库 (Slaves)**：一个或多个，通过 Binlog 异步或半同步地从主库复制数据。它们专门负责处理读操作（SELECT），从而分担主库的压力。




这样的架构，不仅提升了系统的整体吞吐量，还天然具备了一定的容灾能力。一个从库挂了，我们还有其他从库可以顶上。

## 二、Go 代码实现：构建灵活的数据库访问层

理论讲完了，我们来看代码。在 Go 项目中实现读写分离，核心是**在代码层面做好数据源的路由**。也就是说，你的代码需要能智能地判断，当前这个数据库操作，应该发给主库还是从库。

我们不推荐在每个业务逻辑里都去手动选择数据库连接，这样太繁琐且容易出错。更好的方式是封装一个统一的数据访问层。

### 1. 配置先行：在 `go-zero` 中定义多数据源

首先，在我们的微服务配置文件 `etc/config.yaml` 中，需要清晰地定义主库和从库的连接信息。

```yaml
Name: trial-management-api
Host: 0.0.0.0
Port: 8888

# 数据库配置
Database:
  Master:
    DSN: "user:password@tcp(mysql-master:3306)/ctms_db?charset=utf8mb4&parseTime=True&loc=Local"
    MaxOpenConns: 20   # 主库连接数不宜过高，避免写竞争
    MaxIdleConns: 10
    ConnMaxLifetime: 3600 # s
  Slaves:
    - DSN: "user:password@tcp(mysql-slave-1:3306)/ctms_db?charset=utf8mb4&parseTime=True&loc=Local"
    - DSN: "user:password@tcp(mysql-slave-2:3306)/ctms_db?charset=utf8mb4&parseTime=True&loc=Local"
    # 可以配置更多从库
  SlaveMaxOpenConns: 100 # 从库可以承受更高的并发读
  SlaveMaxIdleConns: 20
  SlaveConnMaxLifetime: 3600 # s
```

对应的，`internal/config/config.go` 结构体也要跟上：

```go
package config

import "github.com/zeromicro/go-zero/rest"

type DBConfig struct {
	DSN             string
	MaxOpenConns    int `json:",optional"`
	MaxIdleConns    int `json:",optional"`
	ConnMaxLifetime int `json:",optional"`
}

type DatabaseConfig struct {
	Master              DBConfig
	Slaves              []DBConfig
	SlaveMaxOpenConns   int `json:",optional"`
	SlaveMaxIdleConns   int `json:",optional"`
	SlaveConnMaxLifetime int `json:",optional"`
}

type Config struct {
	rest.RestConf
	Database DatabaseConfig
}
```

### 2. 封装数据源管理器 `DBManager`

这是我们实现读写分离的核心。我们会创建一个 `DBManager` 结构体，它持有主库和所有从库的连接池，并提供获取连接的方法。

```go
// internal/db/manager.go

package db

import (
	"database/sql"
	"math/rand"
	"sync"
	"time"

	_ "github.com/go-sql-driver/mysql"
	"github.com/zeromicro/go-zero/core/logx"
)

type DBManager struct {
	master *sql.DB
	slaves []*sql.DB
	mu     sync.RWMutex
}

// NewDBManager 根据配置初始化管理器
func NewDBManager(masterDSN string, slaveDSNs []string) (*DBManager, error) {
	// 初始化 Master
	master, err := sql.Open("mysql", masterDSN)
	if err != nil {
		logx.Errorf("failed to open master db: %v", err)
		return nil, err
	}
	// TODO: 从配置中读取连接池参数并设置
	// master.SetMaxOpenConns(...)

	if err = master.Ping(); err != nil {
		logx.Errorf("failed to ping master db: %v", err)
		return nil, err
	}

	mgr := &DBManager{
		master: master,
	}

	// 初始化 Slaves
	for _, dsn := range slaveDSNs {
		slave, err := sql.Open("mysql", dsn)
		if err != nil {
			logx.Errorf("failed to open slave db (%s): %v", dsn, err)
			continue // 一个从库失败不影响其他
		}
		// TODO: 从配置中读取连接池参数并设置
		// slave.SetMaxOpenConns(...)

		if err = slave.Ping(); err != nil {
			logx.Errorf("failed to ping slave db (%s): %v", dsn, err)
			continue
		}
		mgr.slaves = append(mgr.slaves, slave)
	}

	if len(mgr.slaves) == 0 {
		logx.Warn("no available slaves, all read operations will be routed to master")
	}

    // 可以在这里启动一个后台 goroutine 定期检查从库健康状况，动态剔除和恢复

	return mgr, nil
}

// GetMaster 获取主库连接
func (m *DBManager) GetMaster() *sql.DB {
	return m.master
}

// GetSlave 获取一个从库连接
func (m *DBManager) GetSlave() *sql.DB {
	m.mu.RLock()
	defer m.mu.RUnlock()

	if len(m.slaves) == 0 {
		// 降级策略：如果没有可用的从库，返回主库用于读取
		return m.master
	}

	// 简单的随机负载均衡策略
	// 生产环境可以考虑更复杂的策略，如轮询、加权轮询等
	return m.slaves[rand.Intn(len(m.slaves))]
}
```

### 3. 集成到 `ServiceContext`

现在，我们将 `DBManager` 注入到 `go-zero` 的 `ServiceContext` 中，这样所有的 `logic` 都可以方便地使用它。

```go
// internal/svc/servicecontext.go

package svc

import (
	"your-project/internal/config"
	"your-project/internal/db"
)

type ServiceContext struct {
	Config    config.Config
	DBManager *db.DBManager
	// ... 其他依赖，比如 Redis 等
}

func NewServiceContext(c config.Config) *ServiceContext {
	// 从配置中提取 DSN 列表
	slaveDSNs := make([]string, 0, len(c.Database.Slaves))
	for _, s := range c.Database.Slaves {
		slaveDSNs = append(slaveDSNs, s.DSN)
	}

	dbMgr, err := db.NewDBManager(c.Database.Master.DSN, slaveDSNs)
	if err != nil {
		panic(err) // 初始化失败，直接 panic
	}

	return &ServiceContext{
		Config:    c,
		DBManager: dbMgr,
	}
}
```

### 4. 在 `Logic` 中使用

万事俱备，只欠东风。现在，在业务逻辑代码中，我们可以非常优雅地实现读写分离了。

**写操作示例：提交一份患者报告**

```go
// internal/logic/patientreport/submitreportlogic.go

package patientreport

import (
	"context"
	// ...
)

type SubmitReportLogic struct {
	logx.Logger
	ctx    context.Context
	svcCtx *svc.ServiceContext
}

// ... NewSubmitReportLogic

func (l *SubmitReportLogic) SubmitReport(req *types.SubmitReportReq) (resp *types.SubmitReportResp, err error) {
	// 获取主库连接
	db := l.svcCtx.DBManager.GetMaster()

	// 执行写入操作
	_, err = db.ExecContext(l.ctx, "INSERT INTO reports (patient_id, content) VALUES (?, ?)", req.PatientID, req.Content)
	if err != nil {
		// 记录错误，返回
		return nil, err
	}
	
	// ...
	return &types.SubmitReportResp{Success: true}, nil
}
```

**读操作示例：获取患者报告列表**

```go
// internal/logic/patientreport/getreportlistlogic.go

package patientreport

import (
	"context"
	// ...
)

type GetReportListLogic struct {
	logx.Logger
	ctx    context.Context
	svcCtx *svc.ServiceContext
}

// ... NewGetReportListLogic

func (l *GetReportListLogic) GetReportList(req *types.GetReportListReq) (resp *types.GetReportListResp, err error) {
	// 获取从库连接
	db := l.svcCtx.DBManager.GetSlave()

	// 执行查询操作
	rows, err := db.QueryContext(l.ctx, "SELECT id, content, created_at FROM reports WHERE patient_id = ?", req.PatientID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var reports []types.Report
	for rows.Next() {
		// ... 扫描数据
	}

	// ...
	return &types.GetReportListResp{Reports: reports}, nil
}
```

看到这里，你会发现，业务代码非常干净。`logic` 层的开发人员只需要关心一件事：**我这个操作是读还是写？** 然后调用 `GetMaster()` 或 `GetSlave()` 即可，完全不用关心底层有多少个数据库、负载均衡是怎么做的。

## 三、踩坑与思考：处理主从延迟问题

读写分离架构最大的一个挑战就是**主从复制延迟**。在我们 ePRO 系统刚上线时，就遇到了一个典型问题：患者提交完日记后，App 立即刷新列表，结果发现刚刚提交的内容不见了。过了两三秒再刷新，又出来了。

这就是因为：
1.  写请求发给了 Master，成功。
2.  App 紧接着的读请求，被路由到了一个还没来得及同步这条新数据的 Slave 上。
3.  Slave 从自己的数据里一查，自然查不到，就返回了空。

这个问题对用户体验是致命的。我们后来总结了几个解决方案：

#### 方案一：强制读主（简单粗暴）

对于那些对实时性要求极高的场景，我们可以临时“破坏”读写分离规则，强制从主库读取数据。

比如，在我们的 `DBManager` 中增加一个方法：

```go
// GetMasterForced 强制获取主库连接，用于读操作
func (m *DBManager) GetMasterForced() *sql.DB {
    return m.master
}
```

在提交报告后的那个刷新列表的接口里，调用 `GetMasterForced()`。

*   **优点**：实现简单，能立竿见影地解决问题。
*   **缺点**：滥用会导致主库压力过大，违背了读写分离的初衷。必须严格控制使用范围。

#### 方案二：写后读主（Session 级别）

一个更优雅的方案是，当一个用户执行了写操作后，在接下来的一小段时间内（比如 30 秒），将这个用户的所有读请求都路由到主库。

这通常需要借助 `middleware` 和 `session/cache` 来实现。

1.  在写操作的接口处理完毕后，在 Redis 中记录一个标志位，如 `set user_read_master:user_id 1 EX 30`。
2.  创建一个中间件，对所有需要数据库操作的请求进行拦截。
3.  中间件检查 Redis 中是否存在该用户的标志位。如果存在，就在 `context` 中注入一个“强制读主”的标记。
4.  `DBManager` 的 `GetSlave()` 方法改造一下，先检查 `context` 中是否有这个标记，如果有，就返回 `master` 连接。

这个方案相对复杂，但能精确控制范围，兼顾了一致性和性能。

## 总结

好了，关于 Go 和 MySQL 的读写分离实践，今天就先聊到这里。回顾一下关键点：

1.  **从业务出发**：分析系统的读写模型，是决定是否采用读写分离的前提。
2.  **封装是关键**：构建一个统一的数据访问层（如 `DBManager`），对业务代码屏蔽底层复杂性。
3.  **配置驱动**：利用 `go-zero` 强大的配置能力，灵活管理多个数据源。
4.  **直面延迟**：主从延迟是必然存在的，必须准备好应对策略，如“强制读主”或“写后读主”，以保证关键业务流程的用户体验。

在临床医疗这个特殊的领域，技术方案的选择总是要服务于数据的安全、准确和系统的稳定。读写分离不是银弹，但它确实是我们在构建高性能、高可用后端服务时，工具箱里一件非常趁手的兵器。希望我的这些一线经验，能对你有所启发。